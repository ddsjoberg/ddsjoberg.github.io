@article{gao2008sample,
  author = {Gao, Ping and Ware, James H. and Mehta, Cyrus},
  year = {2008},
  title = {Sample Size Re-Estimation for Adaptive Sequential Design in Clinical Trials},
  journal = {Journal of Biopharmaceutical Statistics},
  issn = {1054-3406},
  doi = {10.1080/10543400802369053},
  pmid = {18991116},
  url = {http://dx.doi.org/10.1080/10543400802369053},
  volume = {18},
  month = {8},
  pages = {1184--1196},
  number = {6},
  abstract = {There is considerable interest in methods that use accumulated data to modify trial sample size. However, sample size re-estimation in group sequential designs has been controversial. We describe a method for sample size re-estimation at the penultimate stage of a group sequential design that achieves specified power against an alternative hypothesis corresponding to the current point estimate of the treatment effect.}
}
@article{cui1999modification,
  author = {Cui, Lu and Hung, H. M. James and Wang, Sue‐Jane},
  year = {1999},
  title = {Modification of Sample Size in Group Sequential Clinical Trials},
  journal = {Biometrics},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341x.1999.00853.x},
  pmid = {11315017},
  url = {http://dx.doi.org/10.1111/j.0006-341x.1999.00853.x},
  volume = {55},
  pages = {853--857},
  number = {3},
  abstract = {Summary. In group sequential clinical trials, sample size reestimation can be a complicated issue when it allows for change of sample size to be influenced by an observed sample path. Our simulation studies show that increasing sample size based on an interim estimate of the treatment difference can substantially inflate the probability of type I error in most practical situations. A new group sequential test procedure is developed by modifying the weights used in the traditional repeated significance two-sample mean test. The new test has the type I error probability preserved at the target level and can provide a substantial gain in power with the increase of sample size. Generalization of the new procedure is discussed.}
}
@article{chen2004increasing,
  author = {Chen, Y. H. Joshua and DeMets, David L. and Lan, K. K. Gordon},
  year = {2004},
  title = {Increasing the sample size when the unblinded interim result is promising},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.1688},
  pmid = {15057876},
  url = {http://dx.doi.org/10.1002/sim.1688},
  volume = {23},
  month = {4},
  pages = {1023--1038},
  number = {7},
  abstract = {Increasing the sample size based on unblinded interim result may inflate the type I error rate and appropriate statistical adjustments may be needed to control the type I error rate at the nominal level. We briefly review the existing approaches which allow early stopping due to futility, or change the test statistic by using different weights, or adjust the critical value for final test, or enforce rules for sample size recalculation. The implication of early stopping due to futility and a simple modification to the weighted Z-statistic approach are discussed. In this paper, we show that increasing the sample size when the unblinded interim result is promising will not inflate the type I error rate and therefore no statistical adjustment is necessary. The unblinded interim result is considered promising if the conditional power is greater than 50 per cent or equivalently, the sample size increment needed to achieve a desired power does not exceed an upper bound. The actual sample size increment may be determined by important factors such as budget, size of the eligible patient population and competition in the market. The 50 per cent-conditional-power approach is extended to a group sequential trial with one interim analysis where a decision may be made at the interim analysis to stop the trial early due to a convincing treatment benefit, or to increase the sample size if the interim result is not as good as expected. The type I error rate will not be inflated if the sample size may be increased only when the conditional power is greater than 50 per cent. If there are two or more interim analyses in a group sequential trial, our simulation study shows that the type I error rate is also well controlled. Copyright © 2004 John Wiley & Sons, Ltd.}
}
@article{proschan1995designed,
  author = {Proschan, Michael A and Hunsberger, Sally A},
  year = {1995},
  title = {Designed Extension of Studies Based on Conditional Power},
  journal = {Biometrics},
  issn = {0006-341X},
  doi = {10.2307/2533262},
  pmid = {8589224},
  url = {http://dx.doi.org/10.2307/2533262},
  volume = {51},
  pages = {1315},
  number = {4},
  abstract = {We propose a flexible method of extending a study based on conditional power. The possibility for extension when the p value at the planned end is small but not statistically significant is built in to the design of the study. The significance of the treatment difference at the planned end is used to determine the number of additional observations needed and the critical value necessary for use after accruing those additional observations. It may therefore be thought of as a two-stage procedure. Even though the observed treatment difference at stage 1 is used to make decisions, the Type I error rate is protected.}
}
@misc{fda1998guidance,
  author = {FDA},
  year = {1998},
  title = {Guidance for industry: E9 statistical principles for clinical trials},
  url = {https://scholar.google.com/scholar?cluster=18413241584278219048}
}
@article{li2002a,
  author = {Li, Gang and Shih, Weichung J and Xie, Tailiang and Lu, Jiang},
  year = {2002},
  title = {A sample size adjustment procedure for clinical trials based on conditional power},
  journal = {Biostatistics},
  pmid = {12933618},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/12933618},
  volume = {3},
  month = {2},
  pages = {277--287},
  number = {2},
  abstract = {Abstract When designing clinical trials, researchers often encounter the uncertainty in the treatment effect or variability assumptions. Hence the sample size calculation at the planning stage of a clinical trial may also be questionable. Adjustment of the sample size during the mid‐course of a clinical trial has become a popular strategy lately. In this paper we propose a procedure for calculating additional sample size needed based on conditional power,  ...}
}
@article{friede2001a,
  author = {Friede, T and Kieser, M},
  year = {2001},
  title = {A comparison of methods for adaptive sample size adjustment},
  doi = {10.1002/sim.972},
  url = {http://dx.doi.org/10.1002/sim.972},
  month = {1},
  abstract = {Abstract In fixed sample size designs, precise knowledge about the magnitude of the outcome variable's variance in the planning phase of a clinical trial is mandatory for an adequate sample size determination. Wittes and Brittain introduced the internal pilot study design that allows recalculation of the sample size during an ongoing trial using the estimated variance obtained from an interim analysis. However, this procedure requires  ...}
}
@article{gould1992sample,
  author = {Gould, Lawrence A and Shih, Weichung},
  year = {1992},
  title = {Sample size re-estimation without unblinding for normally distributed outcomes with unknown variance},
  journal = {Communications in Statistics - Theory and Methods},
  issn = {0361-0926},
  doi = {10.1080/03610929208830947},
  url = {http://dx.doi.org/10.1080/03610929208830947},
  volume = {21},
  pages = {2833--2853},
  number = {10}
}
@article{xing2005a,
  author = {Xing, Biao and Ganju, Jitendra},
  year = {2005},
  title = {A method to estimate the variance of an endpoint from an on‐going blinded trial},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.2070},
  pmid = {15803440},
  url = {http://dx.doi.org/10.1002/sim.2070},
  volume = {24},
  month = {5},
  pages = {1807--1814},
  number = {12},
  abstract = {Blinded estimation of variance allows for changing the sample size without compromising the integrity of the trial. Some of the methods that estimate the variance in a blinded manner either make untenable assumptions or are only applicable to two-treatment trials. We propose a new method for continuous endpoints that makes minimal assumptions. The method uses the enrollment order of subjects and the randomization block size to estimate the variance. It can be applied to normal or non-normal data, trials with two or more treatments, equal or unequal allocation schemes, fixed or random randomization block sizes, and single or multi-centre trials. The variance estimator is unbiased and performs best when the randomization block size is the smallest. Simulation results suggest that for many commonly used randomization block sizes the proposed estimator is expected to perform well. The proposed method is used to estimate the variance of the endpoint for two trials and is shown to perform well by comparison with its unblinded counterpart. Copyright © 2005 John Wiley & Sons, Ltd.}
}
@article{vickers2008extensions,
  author = {Vickers, Andrew J and Cronin, Angel M and Elkin, Elena B and Gonen, Mithat},
  year = {2008},
  title = {Extensions to decision curve analysis, a novel method for evaluating diagnostic tests, prediction models and molecular markers},
  journal = {BMC Medical Informatics and Decision Making},
  issn = {1472-6947},
  doi = {10.1186/1472-6947-8-53},
  pmid = {19036144},
  url = {http://dx.doi.org/10.1186/1472-6947-8-53},
  volume = {8},
  month = {8},
  pages = {1--17},
  number = {1},
  abstract = {Background Decision curve analysis is a novel method for evaluating diagnostic tests, prediction models and molecular markers. It combines the mathematical simplicity of accuracy measures, such as sensitivity and specificity, with the clinical applicability of decision analytic approaches. Most critically, decision curve analysis can be applied directly to a data set, and does not require the sort of external data on costs, benefits and preferences typically required by traditional decision analytic techniques. Methods In this paper we present several extensions to decision curve analysis including correction for overfit, confidence intervals, application to censored data (including competing risk) and calculation of decision curves directly from predicted probabilities. All of these extensions are based on straightforward methods that have previously been described in the literature for application to analogous statistical techniques. Results Simulation studies showed that repeated 10-fold crossvalidation provided the best method for correcting a decision curve for overfit. The method for applying decision curves to censored data had little bias and coverage was excellent; for competing risk, decision curves were appropriately affected by the incidence of the competing risk and the association between the competing risk and the predictor of interest. Calculation of decision curves directly from predicted probabilities led to a smoothing of the decision curve. Conclusion Decision curve analysis can be easily extended to many of the applications common to performance measures for prediction models. Software to implement decision curve analysis is provided.}
}
@article{vickers2006decision,
  author = {Vickers, Andrew J and Elkin, Elena B},
  year = {2006},
  title = {Decision Curve Analysis: A Novel Method for Evaluating Prediction Models},
  journal = {Medical Decision Making},
  issn = {0272-989X},
  doi = {10.1177/0272989X06295361},
  pmid = {17099194},
  url = {http://dx.doi.org/10.1177/0272989X06295361},
  volume = {26},
  month = {6},
  pages = {565--574},
  number = {6},
  abstract = {Background. Diagnostic and prognostic models are typically evaluated with measures of accuracy that do not address clinical consequences. Decision-analytic techniques allow assessment of clinical outcomes but often require collection of additional information and may be cumbersome to apply to models that yield a continuous result. The authors sought a method for evaluating and comparing prediction models that incorporates clinical consequences, requires only the data set on which the models are tested, and can be applied to models that have either continuous or dichotomous results. Method. The authors describe decision curve analysis, a simple, novel method of evaluating predictive models. They start by assuming that the threshold probability of a disease or event at which a patient would opt for treatment is informative of how the patient weighs the relative harms of a false-positive and a false-negative prediction. This theoretical relationship is then used to derive the net benefit of the model across different threshold probabilities. Plotting net benefit against threshold probability yields the “decision curve.” The authors apply the method to models for the prediction of seminal vesicle invasion in prostate cancer patients. Decision curve analysis identified the range of threshold probabilities in which a model was of value, the magnitude of benefit, and which of several models was optimal. Conclusion. Decision curve analysis is a suitable method for evaluating alternative diagnostic and prognostic strategies that has advantages over other commonly used measures and techniques.}
}
@article{mehta2001flexible,
  author = {Mehta, Cyrus R. and Tsiatis, Anastasios A.},
  year = {2001},
  title = {Flexible Sample Size Considerations Using Information-Based Interim Monitoring},
  journal = {Drug Information Journal},
  issn = {0092-8615},
  doi = {10.1177/009286150103500407},
  url = {http://dx.doi.org/10.1177/009286150103500407},
  volume = {35},
  month = {1},
  pages = {1095--1112},
  number = {4},
  abstract = {At the design phase of a clinical trial the total number of participants needed to detect a clinically important treatment difference with sufficient precision frequently depends on nuisance parameters such as variance, baseline response rate, or regression coefficients other than the main effect. In practical applications, nuisance parameter values are often unreliable guesses founded on little or no available past history. Sample size calculations based on these initial guesses may, therefore, lead to under- or over-powered studies. In this paper, we argue that the precision with which a treatment effect is estimated is directly related to the statistical information in the data. In general, statistical information is a complicated function of sample size and nuisance parameters. However, the amount of information necessary to answer the scientific question concerning treatment difference is easily calculated a priori and applies to almost any statistical model for a large variety of endpoints. It is thus possible to be flexible on sample size but rather continue collecting data until we have achieved the desired information. Such a strategy is well suited to being adopted in conjunction with a group sequential clinical trial where the data are monitored routinely anyway. We present several scenarios and examples of how group sequential information-based design and monitoring can be carried out and demonstrate through simulations that this type of strategy will indeed give us the desired operating characteristics.}
}
@article{proschan2009sample,
  author = {Proschan, Michael A},
  year = {2009},
  title = {Sample size re‐estimation in clinical trials},
  journal = {Biometrical Journal},
  issn = {1521-4036},
  doi = {10.1002/bimj.200800266},
  pmid = {19358221},
  url = {http://dx.doi.org/10.1002/bimj.200800266},
  month = {9},
  number = {2},
  abstract = {Adaptive clinical trials are becoming very popular because of their flexibility in allowing mid-stream changes of sample size, endpoints, populations, etc. At the same time, they have been regarded with mistrust because they can produce bizarre results in very extreme settings. Understanding the advantages and disadvantages of these rapidly developing methods is a must. This paper reviews flexible methods for sample size re-estimation when the outcome is continuous.}
}
@article{proschan2005twostage,
  author = {Proschan, MA},
  year = {2005},
  title = {Two-stage sample size re-estimation based on a nuisance parameter: a review},
  doi = {10.1081/BIP-200062852},
  url = {http://dx.doi.org/10.1081/BIP-200062852},
  month = {5},
  abstract = {Abstract Sample size calculations are important and difficult in clinical trails because they depend on the nuisance parameter and treatment effect. Recently, much attention has been focused on two-stage methods whereby the first stage constitutes an internal pilot study }
}
@article{li2005twostage,
  author = {Li, Gang and Shih, Weichung J and Wang, Yining},
  year = {2005},
  title = {Two-Stage Adaptive Design for Clinical Trials with Survival Data},
  journal = {Journal of Biopharmaceutical Statistics},
  issn = {1054-3406},
  doi = {10.1081/BIP-200062293},
  pmid = {16022174},
  url = {http://dx.doi.org/10.1081/BIP-200062293},
  volume = {15},
  month = {5},
  pages = {707--718},
  number = {4},
  abstract = {In long-term clinical trials we often need to monitor the patients’ enrollment, compliance, and treatment effect during the study. In this paper we take the conditional power approach and consider a two-stage design based on the ideas of Li et al. (2002 Li , G. , Shih , W. J. , Xie , T. , Lu , J. ( 2002 ). A sample size adjustment procedure for clinical trials based on conditional power . Biostatistics 3 : 277 – 287 . [PUBMED] [CROSSREF] [CrossRef], [PubMed], [Web of Science ®]) for trials with survival endpoints. We make projections and decisions regarding the future course of the trial from the interim data. The decision includes possible early termination of the trial for convincing evidence of futility or efficacy, and projection includes how many additional patients are needed to enroll and how long the enrollment and follow-up may be when continuing the trial. The flexibility of the adaptive design is demonstrated by an example, the Coumadin Aspirin Reinfarction Study.}
}
@article{kwak2014phase,
  author = {Kwak, Minjung and Jung, Sin‐Ho},
  year = {2014},
  title = {Phase II clinical trials with time‐to‐event endpoints: optimal two‐stage designs with one‐sample log‐rank test},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.6073},
  pmid = {24338995},
  url = {http://dx.doi.org/10.1002/sim.6073},
  volume = {33},
  pages = {2004--2016},
  number = {12},
  abstract = {Phase II clinical trials are often conducted to determine whether a new treatment is sufficiently promising to warrant a major controlled clinical evaluation against a standard therapy. We consider single-arm phase II clinical trials with right censored survival time responses where the ordinary one-sample logrank test is commonly used for testing the treatment efficacy. For planning such clinical trials, this paper presents two-stage designs that are optimal in the sense that the expected sample size is minimized if the new regimen has low efficacy subject to constraints of the type I and type II errors. Two-stage designs, which minimize the maximal sample size, are also determined. Optimal and minimax designs for a range of design parameters are tabulated along with examples. Copyright © 2013 John Wiley & Sons, Ltd.}
}
@article{mehta2011adaptive,
  author = {Mehta, Cyrus R. and Pocock, Stuart J.},
  year = {2011},
  title = {Adaptive increase in sample size when interim results are promising: A practical guide with examples},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.4102},
  pmid = {22105690},
  url = {http://dx.doi.org/10.1002/sim.4102},
  volume = {30},
  month = {11},
  pages = {3267--3284},
  number = {28},
  abstract = {This paper discusses the benefits and limitations of adaptive sample size re-estimation for phase 3 confirmatory clinical trials. Comparisons are made with more traditional fixed sample and group sequential designs. It is seen that the real benefit of the adaptive approach arises through the ability to invest sample size resources into the trial in stages. The trial starts with a small up-front sample size commitment. Additional sample size resources are committed to the trial only if promising results are obtained at an interim analysis. This strategy is shown through examples of actual trials, one in neurology and one in cardiology, to be more advantageous than the fixed sample or group sequential approaches in certain settings. A major factor that has generated controversy and inhibited more widespread use of these methods has been their reliance on non-standard tests and p-values for preserving the type-1 error. If, however, the sample size is only increased when interim results are promising, one can dispense with these non-standard methods of inference. Therefore, in the spirit of making adaptive increases in trial size more widely appealing and readily implementable we here define those promising circumstances in which a conventional final inference can be performed while preserving the overall type-1 error. Methodological, regulatory and operational issues are examined. Copyright © 2010 John Wiley & Sons, Ltd.}
}
@article{hosmer1995confidence,
  author = {Hosmer, David W and Lemeshow, Stanley},
  year = {1995},
  title = {Confidence interval estimates of an index of quality performance based on logistic regression models},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.4780141909},
  pmid = {8552894},
  url = {http://dx.doi.org/10.1002/sim.4780141909},
  number = {19},
  abstract = {This paper considers an index of hospital quality performance defined as the ratio of the observed number deaths to the number predicted by a fitted logistic regression model. We study tests and confidence intervals under two different scenarios depending on the availability of an estimate of the covariance matrix of the coefficints from the fitted logistic regression model. We propose parametric as well as bootstrap-based confidence intervals. We apply the methods to an analysis of the performance of 27 intensive care units.}
}
@article{su1993linear,
  author = {Su, John Q and Liu, Jun S},
  year = {1993},
  title = {Linear Combinations of Multiple Diagnostic Markers},
  journal = {Journal of the American Statistical Association},
  issn = {0162-1459},
  doi = {10.1080/01621459.1993.10476417},
  url = {http://dx.doi.org/10.1080/01621459.1993.10476417},
  volume = {88},
  pages = {1350--1355},
  number = {424}
}
@article{demler2011equivalence,
  author = {Demler, Olga V and Pencina, Michael J and D'Agostino, Ralph B},
  year = {2011},
  title = {Equivalence of improvement in area under ROC curve and linear discriminant analysis coefficient under assumption of normality},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.4196},
  pmid = {21337594},
  url = {http://dx.doi.org/10.1002/sim.4196},
  volume = {30},
  month = {11},
  pages = {1410--1418},
  number = {12},
  abstract = {In this paper we investigate the addition of new variables to an existing risk prediction model and the subsequent impact on discrimination quantified by the area under the receiver operating characteristics curve (AUC of ROC). Based on practical experience, concerns have emerged that the significance of association of the variable under study with the outcome in the risk model does not correspond to the significance of the change in AUC: that is, often the variable is significant, but the change in AUC is not. This paper demonstrates that under the assumption of multivariate normality and employing linear discriminant analysis (LDA) to construct the risk prediction tool, statistical significance of the new predictor(s) is equivalent to the statistical significance of the increase in AUC. Under these assumptions the result extends asymptotically to logistic regression. We further show that equality of variance–covariance matrices of predictors within cases and non-cases is not necessary when LDA is used. However, our practical example from the Framingham Heart Study data suggests that the finding might be sensitive to the assumption of normality. Copyright © 2011 John Wiley & Sons, Ltd.}
}
@article{wu2015singlearm,
  author = {Wu, Jianrong},
  year = {2015},
  title = {Single-arm Phase II cancer survival trial designs},
  journal = {Journal of Biopharmaceutical Statistics},
  issn = {1054-3406},
  doi = {10.1080/10543406.2015.1052494},
  pmid = {26098141},
  url = {http://dx.doi.org/10.1080/10543406.2015.1052494},
  month = {6},
  pages = {644--56},
  number = {4},
  abstract = {The current practice for designing single-arm Phase II trials with time-to-event endpoints is limited to using either a maximum likelihood estimate test under the exponential model or a naive approach based on dichotomizing the event time at a landmark time point. A trial designed under the exponential model may not be reliable, and the naive approach is inefficient. The modified one-sample log-rank test statistic proposed in this article fills the void. In general, the proposed test can be used to design single-arm Phase II survival trials under any parametric survival distribution. Simulation results showed that it preserves type I error well and provides adequate power for Phase II cancer trial designs with time-to-event endpoints.}
}
@article{menis2014new,
  author = {Menis, Jessica and Hasan, Baktiar and Besse, Benjamin},
  year = {2014},
  title = {New clinical research strategies in thoracic oncology: clinical trial design, adaptive, basket and umbrella trials, new end-points and new evaluations of response},
  journal = {European Respiratory Review},
  issn = {0905-9180},
  doi = {10.1183/09059180.00004214},
  pmid = {25176973},
  url = {http://dx.doi.org/10.1183/09059180.00004214},
  number = {133},
  abstract = {In the genomics era, our main goal should be to identify large and meaningful differences in small, molecularly selected groups of patients. Classical phase I, II and III models for drug development require large resources, limiting the number of experimental agents that can be tested and making the evaluation of targeted agents inefficient. There is an urgent need to streamline the development of new compounds, with the aim of identifying “trials designed to learn”, which could lead to subsequent “trials designed to conclude”. Basket trials are often viewed as parallel phase II trials within the same entity, designed on the basis of a common denominator, which can be a molecular alteration(s). Most basket trials are histology-independent and aberration-specific clinical trials. Umbrella trials are built on a centrally performed molecular portrait and molecularly selected cohorts with matched drugs, and can include patients’ randomisation and strategy validation. Beyond new designs, new end-points and new evaluation techniques are also warranted to finally achieve methodology and clinical improvements, in particular within immunotherapy trials.}
}
@article{robins1992information,
  author = {Robins, J M},
  year = {1992},
  title = {Information recovery and bias adjustment in proportional hazards regression analysis of randomized trials using surrogate markers},
  journal = {Proceedings of the Biopharmaceutical Section, American Statistical Association},
  publisher = {Proceedings of the Biopharmaceutical Section, American Statistical Association},
  volume = {24},
  number = {3}
}
@article{satten2001the,
  author = {Satten, Glen A and Datta, Somnath},
  year = {2001},
  title = {The Kaplan–Meier Estimator as an Inverse-Probability-of-Censoring Weighted Average},
  journal = {The American Statistician},
  issn = {0003-1305},
  doi = {10.1198/000313001317098185},
  url = {http://dx.doi.org/10.1198/000313001317098185},
  volume = {55},
  month = {1},
  pages = {207--210},
  number = {3}
}
@article{robins1993information,
  author = {Robins, James M},
  year = {1993},
  title = {Information recovery and bias adjustment in proportional hazards regression analysis of randomized trials using surrogate markers},
  journal = {Proceedings of the Biopharmaceutical Section, American Statistical Association},
  url = {https://scholar.google.com/scholar?cluster=17998251820933539173},
  volume = {24}
}
@article{cunanan2017an,
  author = {Cunanan, Kristen M and Iasonos, Alexia and Shen, Ronglai and Begg, Colin B and Gönen, Mithat},
  year = {2017},
  title = {An efficient basket trial design},
  journal = {Statistics in Medicine},
  issn = {1097-0258},
  doi = {10.1002/sim.7227},
  pmid = {28098411},
  url = {http://dx.doi.org/10.1002/sim.7227},
  volume = {36},
  pages = {1568--1579},
  number = {10},
  abstract = {The landscape for early phase cancer clinical trials is changing dramatically because of the advent of targeted therapy. Increasingly, new drugs are designed to work against a target such as the presence of a specific tumor mutation. Because typically only a small proportion of cancer patients will possess the mutational target, but the mutation is present in many different cancers, a new class of basket trials is emerging, whereby the drug is tested simultaneously in different baskets, that is, subgroups of different tumor types. Investigators desire not only to test whether the drug works but also to determine which types of tumors are sensitive to the drug. A natural strategy is to conduct parallel trials, with the drug s effectiveness being tested separately, using for example, the popular Simon two-stage design independently in each basket. The work presented is motivated by the premise that the efficiency of this strategy can be improved by assessing the homogeneity of the baskets response rates at an interim analysis and aggregating the baskets in the second stage if the results suggest the drug might be effective in all or most baskets. Via simulations, we assess the relative efficiencies of the two strategies. Because the operating characteristics depend on how many tumor types are sensitive to the drug, there is no uniformly efficient strategy. However, our investigation demonstrates that substantial efficiencies are possible if the drug works in most or all baskets, at the cost of modest losses of power if the drug works in only a single basket. Copyright © 2017 John Wiley & Sons, Ltd.}
}
@article{cunanan2016basket,
  author = {Cunanan, Kristen M and Gonen, Mithat and Shen, Ronglai and Hyman, David M and Riely, Gregory J and Begg, Colin B and Iasonos, Alexia},
  year = {2016},
  title = {Basket Trials in Oncology: A Trade-Off Between Complexity and Efficiency},
  journal = {Journal of Clinical Oncology},
  issn = {0732-183X},
  doi = {10.1200/JCO.2016.69.9751},
  pmid = {27893325},
  url = {http://dx.doi.org/10.1200/JCO.2016.69.9751},
  volume = {35},
  pages = {271--273},
  number = {3},
  abstract = {The current oncology drug development landscape is dominated by efforts to create therapies that are mechanistically designed to improve outcomes for patients with cancers that harbor specific molecular aberrations, which often occur across a variety of tumor types. In the evaluation of targeted therapies, basket trials have emerged as an approach to test the hypothesis that targeted therapies may be effective independent of tumor histology, as  ...}
}
@article{cunanan2016an,
  author = {Cunanan, K and Iasonos, A and Shen, R and Begg, CB and Gonen, M},
  year = {2016},
  title = {An Efficient Basket Trial Design},
  url = {https://scholar.google.com/scholar?cluster=760293664491964859},
  abstract = {Abstract The landscape for early phase cancer clinical trials is changing dramatically due to the advent of targeted therapy. Increasingly, new drugs are designed to work against a target such as the presence of a specific tumor mutation. Since typically only a small proportion of cancer patients will possess the mutational target, but the mutation is present in many different cancers, a new class of basket trials is emerging, whereby the drug is  ...}
}
@article{redig2015basket,
  author = {Redig, Amanda J and Jänne, Pasi A},
  year = {2015},
  title = {Basket Trials and the Evolution of Clinical Trial Design in an Era of Genomic Medicine},
  journal = {Journal of Clinical Oncology},
  issn = {0732-183X},
  doi = {10.1200/JCO.2014.59.8433},
  pmid = {25667288},
  url = {http://dx.doi.org/10.1200/JCO.2014.59.8433},
  volume = {33},
  pages = {975--977},
  number = {9},
  abstract = {Since the days of the ancient Greeks, the pathologic hallmarks of malignancy have been reflected in the language of oncology. Hippocrates was the first to use carcinoma—or crab—to describe the familiar invading sweep of tumor cells across tissue planes, and several hundred years later, Galen described the oncos—or swelling—of tumors from which the field of oncology takes its name. However, although the histopathology of malignancy has  ...}
}
@article{austin2017accounting,
  author = {Austin, Peter C and Fine, Jason P},
  year = {2017},
  title = {Accounting for competing risks in randomized controlled trials: a review and recommendations for improvement.},
  journal = {Statistics in medicine},
  issn = {0277-6715},
  doi = {10.1002/sim.7215},
  pmid = {28102550},
  url = {http://dx.doi.org/10.1002/sim.7215},
  volume = {36},
  pages = {1203--1209},
  number = {8},
  abstract = {In studies with survival or time-to-event outcomes, a competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. Specialized statistical methods must be used to analyze survival data in the presence of competing risks. We conducted a review of randomized controlled trials with survival outcomes that were published in high-impact general medical journals. Of 40 studies that we identified, 31 (77.5%) were potentially susceptible to competing risks. However, in the majority of these studies, the potential presence of competing risks was not accounted for in the statistical analyses that were described. Of the 31 studies potentially susceptible to competing risks, 24 (77.4%) reported the results of a Kaplan-Meier survival analysis, while only five (16.1%) reported using cumulative incidence functions to estimate the incidence of the outcome over time in the presence of competing risks. The former approach will tend to result in an overestimate of the incidence of the outcome over time, while the latter approach will result in unbiased estimation of the incidence of the primary outcome over time. We provide recommendations on the analysis and reporting of randomized controlled trials with survival outcomes in the presence of competing risks. &#xA9; 2017 The Authors. Statistics in Medicine published by John Wiley &amp; Sons Ltd.}
}
@article{laan2005direct,
  author = {Laan, MJ van der and Petersen, ML},
  year = {2005},
  title = {Direct effect models},
  url = {https://scholar.google.com/scholar?cluster=17792324999594208280},
  month = {5},
  abstract = {Abstract The causal effect of a treatment on an outcome is generally mediated by several intermediate variables. Estimation of the component of the causal effect of a treatment that is mediated by a given intermediate variable (the indirect effect of the treatment), and the }
}
@article{laan2007statistical,
  author = {Laan, Mark J van der and Petersen, Maya L},
  year = {2007},
  title = {Statistical learning of origin-specific statically optimal individualized treatment rules.},
  journal = {The international journal of biostatistics},
  issn = {1557-4679},
  pmid = {19122792},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/19122792},
  volume = {3},
  month = {7},
  pages = {Article 6},
  number = {1},
  abstract = {Consider a longitudinal observational or controlled study in which one collects chronological data over time on a random sample of subjects. The time-dependent process one observes on each subject contains time-dependent covariates, time-dependent treatment actions, and an outcome process or single final outcome of interest. A statically optimal individualized treatment rule (as introduced in van der Laan et. al. (2005), Petersen et. al. (2007)) is a treatment rule which at any point in time conditions on a user-supplied subset of the past, computes the future static treatment regimen that maximizes a (conditional) mean future outcome of interest, and applies the first treatment action of the latter regimen. In particular, Petersen et. al. (2007) clarified that, in order to be statically optimal, an individualized treatment rule should not depend on the observed treatment mechanism. Petersen et. al. (2007) further developed estimators of statically optimal individualized treatment rules based on a past capturing all confounding of past treatment history on outcome. In practice, however, one typically wishes to find individualized treatment rules responding to a user-supplied subset of the complete observed history, which may not be sufficient to capture all confounding. The current article provides an important advance on Petersen et. al. (2007) by developing locally efficient double robust estimators of statically optimal individualized treatment rules responding to such a user-supplied subset of the past. However, failure to capture all confounding comes at a price; the static optimality of the resulting rules becomes origin-specific. We explain origin-specific static optimality, and discuss the practical importance of the proposed methodology. We further present the results of a data analysis in which we estimate a statically optimal rule for switching antiretroviral therapy among patients infected with resistant HIV virus.}
}
@article{laan2007causal,
  author = {Laan, Mark J van der and Petersen, Maya L},
  year = {2007},
  title = {Causal Effect Models for Realistic Individualized Treatment and Intention to Treat Rules},
  journal = {The International Journal of Biostatistics},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1022},
  pmid = {19122793},
  url = {http://dx.doi.org/10.2202/1557-4679.1022},
  volume = {3},
  month = {7},
  pages = {Article 3},
  number = {1},
  abstract = {Marginal structural models (MSM) are an important class of models in causal inference. Given a longitudinal data structure observed on a sample of n independent and identically distributed experimental units, MSM model the counterfactual outcome distribution corresponding with a static treatment intervention, conditional on user-supplied baseline covariates. Identification of a static treatment regimen-specific outcome distribution based on observational data requires, beyond the standard sequential randomization assumption, the assumption that each experimental unit has positive probability of following the static treatment regimen. The latter assumption is called the experimental treatment assignment (ETA) assumption, and is parameter-specific. In many studies the ETA is violated because some of the static treatment interventions to be compared cannot be followed by all experimental units, due either to baseline characteristics or to the occurrence of certain events over time. For example, the development of adverse effects or contraindications can force a subject to stop an assigned treatment regimen.In this article we propose causal effect models for a user-supplied set of realistic individualized treatment rules. Realistic individualized treatment rules are defined as treatment rules which always map into the set of possible treatment options. Thus, causal effect models for realistic treatment rules do not rely on the ETA assumption and are fully identifiable from the data. Further, these models can be chosen to generalize marginal structural models for static treatment interventions. The estimating function methodology of Robins and Rotnitzky (1992) (analogue to its application in Murphy, et. al. (2001) for a single treatment rule) provides us with the corresponding locally efficient double robust inverse probability of treatment weighted estimator.In addition, we define causal effect models for “intention-to-treat” regimens. The proposed intention-to-treat interventions enforce a static intervention until the time point at which the next treatment does not belong to the set of possible treatment options, at which point the intervention is stopped. We provide locally efficient estimators of such intention-to-treat causal effects.}
}
@article{laan2002locally,
  author = {Laan, Mark J van der and Hubbard, Alan E and Robins, James M},
  year = {2002},
  title = {Locally Efficient Estimation of a Multivariate Survival Function in Longitudinal Studies},
  journal = {Journal of the American Statistical Association},
  issn = {0162-1459},
  doi = {10.1198/016214502760047023},
  url = {http://dx.doi.org/10.1198/016214502760047023},
  volume = {97},
  month = {2},
  pages = {494--507},
  number = {458}
}
@article{kattan2003a,
  author = {Kattan, Michael W and Heller, Glenn and Brennan, Murray F},
  year = {2003},
  title = {A competing-risks nomogram for sarcoma-specific death following local recurrence.},
  journal = {Statistics in medicine},
  issn = {0277-6715},
  doi = {10.1002/sim.1574},
  pmid = {14601016},
  url = {http://dx.doi.org/10.1002/sim.1574},
  volume = {22},
  month = {3},
  pages = {3515--25},
  number = {22},
  abstract = {The majority of staging systems focus on the definition of stage, and, therefore, prediction of prognosis. In the current era of clinical trial research, it has become apparent that the clinical stage alone is not sufficient to assess patient risk of treatment failure. As the number of biological markers increases, our ability to partition the traditional disease classification system improves, and our ability to predict patient success continues to increase. One approach to quantifying individual patient risk is through the nomogram. Nomograms are graphical representations of statistical models, which provide the probability of treatment outcome based on patient-specific covariates. We will focus on the use of the nomogram when the response variable is time to failure and there are multiple, possibly dependent, competing causes of failure. In this setting, estimation of the failure probability through direct application of the Cox proportional hazards model provides the probability of failure (for example, death from cancer) assuming failure from a dependent competing cause will not occur. In many clinical settings this is an unrealistic assumption. The purpose of this study is to illustrate the use of the conditional cumulative incidence function for providing a patient-specific prediction of the probability of failure in the setting of competing risks. A competing risks nomogram is produced to estimate the probability of death due to sarcoma for patients who have already developed a local recurrence of their initially treated soft-tissue sarcoma.}
}
@article{meiramachado2009multistate,
  author = {Meira-Machado, Luís and Uña-Álvarez, Jacobo de and Cadarso-Suárez, Carmen and Andersen, Per K},
  year = {2009},
  title = {Multi-state models for the analysis of time-to-event data},
  journal = {Statistical Methods in Medical Research},
  issn = {0962-2802},
  doi = {10.1177/0962280208092301},
  pmid = {18562394},
  url = {http://dx.doi.org/10.1177/0962280208092301},
  volume = {18},
  month = {9},
  pages = {195--222},
  number = {2},
  abstract = {The experience of a patient in a survival study may be modelled as a process with two states and one possible transition from an “alive” state to a “dead” state. In some studies, however, the “alive” state may be partitioned into two or more intermediate (transient) states, each of which corresponding to a particular stage of the illness. In such studies, multi-state models can be used to model the movement of patients among the various states. In these models issues, of interest include the estimation of progression rates, assessing the effects of individual risk factors, survival rates or prognostic forecasting. In this article, we review modelling approaches for multi-state models, and we focus on the estimation of quantities such as the transition probabilities and survival probabilities. Differences between these approaches are discussed, focussing on possible advantages and disadvantages for each method. We also review the existing software currently available to fit the various models and present new software developed in the form of an R library to analyse such models. Different approaches and software are illustrated using data from the Stanford heart transplant study and data from a study on breast cancer conducted in Galicia, Spain.}
}
@article{putter2007tutorial,
  author = {Putter, H and Fiocco, M and Geskus, RB},
  year = {2007},
  title = {Tutorial in biostatistics: competing risks and multi-state models.},
  journal = {Statistics in medicine},
  issn = {0277-6715},
  doi = {10.1002/sim.2712},
  pmid = {17031868},
  url = {http://dx.doi.org/10.1002/sim.2712},
  volume = {26},
  month = {7},
  pages = {2389--430},
  number = {11},
  abstract = {Standard survival data measure the time span from some time origin until the occurrence of one type of event. If several types of events occur, a model describing progression to each of these competing risks is needed. Multi-state models generalize competing risks models by also describing transitions to intermediate events. Methods to analyze such models have been developed over the last two decades. Fortunately, most of the analyzes can be performed within the standard statistical packages, but may require some extra effort with respect to data preparation and programming. This tutorial aims to review statistical methods for the analysis of competing risks and multi-state models. Although some conceptual issues are covered, the emphasis is on practical issues like data preparation, estimation of the effect of covariates, and estimation of cumulative incidence functions and state and transition probabilities. Examples of analysis with standard software are shown.}
}
@article{satagopan2004a,
  author = {Satagopan, JM and Ben-Porat, L and Berwick, M and Robson, M and Kutler, D and Auerbach, AD},
  year = {2004},
  title = {A note on competing risks in survival data analysis},
  journal = {British Journal of Cancer},
  issn = {0007-0920},
  doi = {10.1038/sj.bjc.6602102},
  pmid = {15305188},
  url = {http://dx.doi.org/10.1038/sj.bjc.6602102},
  volume = {91},
  month = {4},
  pages = {1229--1235},
  number = {7},
  abstract = {Survival analysis encompasses investigation of time to event data. In most clinical studies, estimating the cumulative incidence function (or the probability of experiencing an event by a given time) is of primary interest. When the data consist of patients who experience an event and censored individuals, a nonparametric estimate of the cumulative incidence can be obtained using the Kaplan–Meier method. Under this approach, the censoring mechanism is assumed to be noninformative. In other words, the survival time of an individual (or the time at which a subject experiences an event) is assumed to be independent of a mechanism that would cause the patient to be censored. Often times, a patient may experience an event other than the one of interest which alters the probability of experiencing the event of interest. Such events are known as competing risk events. In this setting, it would often be of interest to calculate the cumulative incidence of a specific event of interest. Any subject who does not experience the event of interest can be treated as censored. However, a patient experiencing a competing risk event is censored in an informative manner. Hence, the Kaplan–Meier estimation procedure may not be directly applicable. The cumulative incidence function for an event of interest must be calculated by appropriately accounting for the presence of competing risk events. In this paper, we illustrate nonparametric estimation of the cumulative incidence function for an event of interest in the presence of competing risk events using two published data sets. We compare the resulting estimates with those obtained using the Kaplan–Meier approach to demonstrate the importance of appropriately estimating the cumulative incidence of an event of interest in the presence of competing risk events.}
}
@article{andersen2002multistate,
  author = {Andersen, Per and Keiding, Niels},
  year = {2002},
  title = {Multi-state models for event history analysis},
  journal = {Statistical Methods in Medical Research},
  issn = {0962-2802},
  doi = {10.1191/0962280202SM276ra},
  pmid = {12040698},
  url = {http://dx.doi.org/10.1191/0962280202SM276ra},
  volume = {11},
  month = {2},
  pages = {91--115},
  number = {2},
  abstract = {An introduction to event history analysis via multi-state models is given. Examples include the two-state model for survival analysis, the competing risks and illness-death models, and models for bone marrow transplantation. Statistical model specification via transition intensities and likelihood inference is introduced. Consequences of observational patterns are discussed, and a real example concerning mortality and bleeding episodes in a liver cirrhosis trial is discussed.}
}
@article{heller2007improving,
  author = {Heller, Glenn and Kattan, Michael W and Scher, Howard I},
  year = {2007},
  title = {Improving the Decision to Pursue a Phase 3 Clinical Trial by Adjusting for Patient-Specific Factors in Evaluating Phase 2 Treatment Efficacy Data},
  journal = {Medical Decision Making},
  issn = {0272-989X},
  doi = {10.1177/0272989X07303826},
  pmid = {17761958},
  url = {http://dx.doi.org/10.1177/0272989X07303826},
  volume = {27},
  month = {7},
  pages = {380--386},
  number = {4},
  abstract = {Phase 2 clinical trials are undertaken to provide evidence of treatment efficacy and safety. A test statistic that accounts for individual patient risk in the patient population is proposed and applied to a phase 2 clinical trial for castrate metastatic prostate cancer. The test statistic is computed to compare, for each patient, the observed 2-year survival outcome to the predicted 2-year survival probability. A logistic regression model, developed using historical data in the same patient population, is used to adjust for patient risk in predicting the 2-year survival probability. Goodness-of-fit procedures are performed to ensure that a proper model is fit to the data. The test result is compared to the score test, the binomial exact test, and Fisher's exact test, all of which use the average 2-year survival probability in the population as the parameter of interest. The results demonstrate the benefit of risk adjustment in determining treatment efficacy in a single-arm phase 2 trial. By adjusting for patient risk, this method can provide a more precise assessment of phase 2 treatment efficacy, thereby improving the decision whether to proceed to a phase 3 clinical trial.}
}
@article{moore2009increasing,
  author = {Moore, KL and Laan, MJ van der},
  year = {2009},
  title = {Increasing Power in Randomized Trials with Right Censored Outcomes Through Covariate Adjustment},
  journal = {Journal of Biopharmaceutical Statistics},
  issn = {1054-3406},
  doi = {10.1080/10543400903243017},
  pmid = {20183467},
  url = {http://dx.doi.org/10.1080/10543400903243017},
  volume = {19},
  month = {9},
  pages = {1099--1131},
  number = {6},
  abstract = {Targeted maximum likelihood methodology is applied to provide a test that makes use of the covariate data that are commonly collected in randomized trials, and does not require assumptions beyond those of the logrank test when censoring is uninformative. Under informative censoring, the logrank test is biased, whereas the test provided in this article is consistent under consistent estimation of the censoring mechanism or the conditional hazard for survival. Two approaches based on this methodology are provided: (1) a substitution-based approach that targets treatment and time-specific survival from which the logrank parameter is estimated, and (2) directly targeting the logrank parameter.}
}
@article{robins2000correcting,
  author = {Robins, JM and Finkelstein, DM},
  year = {2000},
  title = {Correcting for noncompliance and dependent censoring in an AIDS Clinical Trial with inverse probability of censoring weighted (IPCW) log-rank tests.},
  journal = {Biometrics},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2000.00779.x},
  pmid = {10985216},
  url = {http://dx.doi.org/10.1111/j.0006-341X.2000.00779.x},
  volume = {56},
  pages = {779--88},
  number = {3},
  abstract = {AIDS Clinical Trial Group (ACTG) randomized trial 021 compared the effect of bactrim versus aerosolized pentamidine (AP) as prophylaxis therapy for pneumocystis pneumonia (PCP) in AIDS patients. Although patients randomized to the bactrim arm experienced a significant delay in time to PCP, the survival experience in the two arms was not significantly different (p = .32). In this paper, we present evidence that bactrim therapy improves survival but that the standard intent-to-treat comparison failed to detect this survival advantage because a large fraction of the subjects either crossed over to the other therapy or stopped therapy altogether. We obtain our evidence of a beneficial bactrim effect on survival by artificially regarding the subjects as dependently censored at the first time the subject either stops or switches therapy; we then analyze the data with the inverse probability of censoring weighted Kaplan-Meier and Cox partial likelihood estimators of Robins (1993, Proceedings of the Biopharmaceutical Section, American Statistical Association, pp. 24-33) that adjust for dependent censoring by utilizing data collected on time-dependent prognostic factors.}
}
